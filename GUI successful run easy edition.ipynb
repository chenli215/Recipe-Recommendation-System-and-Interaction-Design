{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import pyrsm as rsm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RAW_interactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1cdc1ce085fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minteraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RAW_interactions.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrecipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RAW_recipes.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RAW_interactions.csv'"
     ]
    }
   ],
   "source": [
    "    \n",
    "interaction = pd.read_csv('data/RAW_interactions.csv')\n",
    "recipe = pd.read_csv(\"data/RAW_recipes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10061]\n",
      "[nltk_data]     由于目标计算机积极拒绝，无法连接。>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     由于目标计算机积极拒绝，无法连接。>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# drop rows with na\n",
    "interaction = interaction.dropna()\n",
    "# reset_index since we dropped some rows\n",
    "interaction = interaction.reset_index()\n",
    "# drop unneeded col\n",
    "interaction = interaction.drop(columns=[\"index\"])\n",
    "# take first 100 rows for example\n",
    "interaction_tmp = interaction.loc[0:99, :]\n",
    "# groupby recipe_id\n",
    "interaction_groupby = interaction.groupby(by=[\"recipe_id\"])[\"review\"].count().sort_values(ascending=False)\n",
    "# transform groupbyed result to dataframe\n",
    "tmp = interaction_groupby.to_frame()\n",
    "tmp = tmp.rename(columns={\"review\": \"review_count\"})\n",
    "# see how many recipes are there\n",
    "len(tmp.query(\"review_count > 80\")) # 1067\n",
    "# if > 80, assgin 1 in the selected col\n",
    "tmp[\"selected\"] = rsm.ifelse(tmp.review_count > 80, 1, 0)\n",
    "# reset_index\n",
    "tmp = tmp.reset_index()\n",
    "# merge\n",
    "interaction_merge = interaction.merge(tmp, on=\"recipe_id\", how=\"left\") # left_join\n",
    "# get selected data only\n",
    "interaction_selected = interaction_merge.query(\"selected == 1\")\n",
    "# reset_index\n",
    "interaction_selected = interaction_selected.reset_index()\n",
    "# drop unneeded col\n",
    "interaction_selected = interaction_selected.drop(columns=[\"index\"])\n",
    "# model to preprocessing reviews\n",
    "\n",
    "nltk.download('punkt') # downloads you a model                                                                                                         \n",
    "nltk.download('stopwords') \n",
    "ps = PorterStemmer() \n",
    "# return a list of tokens\n",
    "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
    "    # step 1: get sentences\n",
    "    sentences = sent_tokenize(doc)\n",
    "    # step 2: get tokens\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        # step 3 (optional): stemming\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        if need_sent:\n",
    "            tokens.append(words)\n",
    "        else:\n",
    "            tokens += words\n",
    "    return [w.lower() for w in tokens if w.isalpha()]\n",
    "review_split_100 = interaction_selected.iloc[:100].review.apply(pre_processing_by_nltk)\n",
    "#read two txt to recognize pos or neg words\n",
    "pos_words = open(\"positive-words.txt\").read().split('\\n')\n",
    "neg_words = open(\"negative-words.txt\").read().split('\\n')\n",
    "#function to calculate percentages\n",
    "def percentage_PN(re): \n",
    "    #calculate positive word\n",
    "    numPosWords = 0\n",
    "    for word in re:\n",
    "        if word in pos_words:\n",
    "            numPosWords += 1\n",
    "    \n",
    "    # calculate negative words    \n",
    "    numNegWords = 0   \n",
    "    for word in re:\n",
    "        if word in neg_words:\n",
    "            numNegWords += 1\n",
    "    \n",
    "    #decide if positiv eor negative\n",
    "    if numPosWords > numNegWords:\n",
    "        return(\"Positive\")\n",
    "    elif numNegWords > numPosWords:\n",
    "        return(\"Negative\")\n",
    "    elif numNegWords == numPosWords:\n",
    "        return(\"Neutral\") \n",
    "# apply on whole data\n",
    "per_results = review_split_100.apply(percentage_PN)\n",
    "#combine percentage results with recipe id\n",
    "d = {'recipe_id':interaction.recipe_id ,'per_results':per_results}\n",
    "df_per = pd.DataFrame(data = d)\n",
    "df_per = df_per.dropna()\n",
    "# combine num_df_per with recipe to get final dataset\n",
    "df_per = df_per.rename(columns={\"recipe_id\": \"id\"})\n",
    "# final dataset\n",
    "DF = df_per.merge(recipe, on=\"id\", how=\"left\") # left_join\n",
    "# specify X and y\n",
    "X, y = DF.loc[:, [\"minutes\", \"n_steps\", \"n_ingredients\"]], DF[\"per_results\"]\n",
    "# splitting the train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=False)\n",
    "# final model\n",
    "clf_cv = MLPClassifier(\n",
    "    activation=\"tanh\",\n",
    "    solver=\"lbfgs\",\n",
    "    alpha=0.01,\n",
    "    hidden_layer_sizes=(4,4),\n",
    "    random_state=1234,\n",
    "    max_iter=10000,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect x\n",
    "tmp = X_train.copy()\n",
    "tmp = tmp.loc[0:0, :]\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "# 第1步，实例化object，建立窗口window\n",
    "window = tk.Tk()\n",
    " \n",
    "# 第2步，给窗口的可视化起名字\n",
    "window.title('Recipe Sentiment Prediction')\n",
    " \n",
    "# 第3步，设定窗口的大小(长 * 宽)\n",
    "window.geometry('600x600')  # 这里的乘是小x\n",
    "\n",
    "# 第4步，加载 wellcome image\n",
    "canvas = tk.Canvas(window, width=500, height=380, bg='yellow')\n",
    "image_file = tk.PhotoImage(file='picture.png')\n",
    "image = canvas.create_image(230, 0, anchor='n', image=image_file)\n",
    "canvas.pack(side='top')\n",
    "tk.Label(window, text='Recipe Review Prediction',font=('Arial', 20)).pack()\n",
    "\n",
    "\n",
    "# 第5步，在图形界面上设定输入框控件entry并放置控件\n",
    "e1 = tk.Entry(window, show=None, font=('Arial', 15))  # 显示成明文形式\n",
    "\n",
    "e2 = tk.Entry(window, show=None, font=('Arial', 15))  # 显示成明文形式\n",
    "\n",
    "e3 = tk.Entry(window, show=None, font=('Arial', 15))  # 显示成明文形式\n",
    "\n",
    "\n",
    "# def \n",
    "def to_predict():\n",
    "    a = e1.get()\n",
    "    a = int(a)\n",
    "    \n",
    "    b = e2.get()\n",
    "    b = int(b)\n",
    "    \n",
    "    c = e3.get()\n",
    "    c = int(c)\n",
    "    \n",
    "    #input variables\n",
    "    tmp.loc[0, \"minutes\"] = a\n",
    "    tmp.loc[0, \"n_steps\"] = b\n",
    "    tmp.loc[0, \"n_ingredients\"] = c\n",
    "    \n",
    "    result = clf_cv.predict(tmp)\n",
    "    \n",
    "    t.insert('insert', result)\n",
    "    \n",
    "tk.Label(window, text='minutes:', font=('Arial', 12)).place(x=10, y=430)    \n",
    "tk.Label(window, text='n_steps:', font=('Arial', 12)).place(x=10, y=460)  \n",
    "tk.Label(window, text='n_ingredients:', font=('Arial', 12)).place(x=10, y=490)  \n",
    "\n",
    "button = tk.Button(window, command = to_predict, text = \"predict\")\n",
    "\n",
    "# pack\n",
    "e1.pack()\n",
    "e2.pack()\n",
    "e3.pack()\n",
    "\n",
    "button.pack()\n",
    "\n",
    "t = tk.Text(window, height=3)\n",
    "t.pack()\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
